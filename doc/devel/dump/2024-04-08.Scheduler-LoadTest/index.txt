Scheduler Load and Stress Testing
=================================
:date: 2024-04-08

Following the initial integration, an extended series of Load- and Stress-testing
was conducted to determine the limitations and weaknesses of the new Scheduler
implementation. During that time, from December to April, several bugs and problems
were identified, improving the maturity of design and code. In the first phase,
the goal was to subject the implementation to increasingly challenging load patterns.
In the second phase, several instrumentation- and measurement methods were developed,
to establish characteristic traits and boudary conditions quantitatively.

This directory holds a loose collection of material related to these testing efforts.
Trace-dumps were produced by adding print statements at relevant locations; over time,
a change set with a suitable setup of such diagnostic print statements was assembled,
which can be applied by `git cherry-pick`. Moreover, load patterns generated by the
`TestChainLoad` tooling can be visualised as _graphviz diagram_ and the collection
of tools in the `StressTestRig` generate report output and data visualisation as
_Gnuplot_ script. Raw measurment data is stored as CSV (see 'csv.hpp').


Load Peak Testing
-----------------

Dump-10::
 Example of a typical clean run working through a homogenous load peak.
 With load patterns in this category, contention is not an issue and, after the
 initial ramp-up, all workers are fully loaded and immediately comence with the
 next job in row, typically requiring ~70µs for the administrative work between
 processing jobs (Note: all those measurements were done with *debug builds*,
 without compiler optimisation). As an artefact of the measurement setup, a
 follow-up job is wired as dependency on each work job; the dependency wiring
 and dependency checking is performed outside the active measurement interval,
 so that only the additional post of the follow-up check adds a constant
 administrative overhead to each job, which can be guessed to be +30µs.

Graph-10::
 Plot from a parametric measurement series with the same settings as used for
 the Dump-10; settings in these range are known to produce clean processing
 with no obivous irregular overhead. In this case here
+
- the series is comprised of 40 measurements, covering a parameter range 10...100
- the free parameter is the number of jobs in a homogenous load peak
- through built-in instrumentation, the job activation can be observed, allowing
  to derive the average work job time, average time in full concurrency and amount
  of time with limitations (less than two active workers)
+
Notably the concurrency approaches the theoretic limit with increasing peak length,
yet some headroom remains. However, by logical reasoning it becomes clear that there
_must be a startup and shutdown phase_ in each run, and this phase must be calculated
with one Job length each (in this case 2ms startup and 2ms shutdown); startup counts
as _one Job completed,_ while shutdown counts as _N-1 Jobs completed._ Taking these
considerations into account, 4ms of the socket indicated by the linear model can be
explained, so there remains a further socket of 5ms. Various observations seem to
indicate that these *5ms* can be considered a *generic scheduling latency*
+
The actual job times are always slightly above the calibrated value (the load operation
is calibrated before each test on the specific hardware; it could be shown that this
calibration also holds under multithreaded performance, but the behaviour degrades
when contention, lock coordination and cache misses come into play)
